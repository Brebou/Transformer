{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef82019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model_Blocks.Transformer as tfm\n",
    "import Data_handler as dh\n",
    "import Training as tr\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98018f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, vec_en, vec_fr, max_len_en, max_len_fr = dh.prepare_data()\n",
    "padding_fr = vec_fr.vocabulary_['PPPading']\n",
    "padding_en = vec_en.vocabulary_['PPPading']\n",
    "len_fr_vocab = len(vec_fr.vocabulary_)\n",
    "len_en_vocab = len(vec_en.vocabulary_)\n",
    "\n",
    "train_dataset,test_dataset = train_test_split(df, test_size=0.16, random_state=23)\n",
    "\n",
    "train_dataset = dh.VocabDataset(train_dataset, padding_fr, padding_en)\n",
    "test_dataset = dh.VocabDataset(test_dataset, padding_fr, padding_en)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990c9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfm.Transformer(len_en_vocab, len_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf08cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100/3566, Loss: 2.0129\n",
      "  Batch 200/3566, Loss: 1.3976\n",
      "  Batch 300/3566, Loss: 1.0435\n",
      "  Batch 400/3566, Loss: 1.0920\n",
      "  Batch 500/3566, Loss: 0.8603\n",
      "  Batch 600/3566, Loss: 0.9050\n",
      "  Batch 700/3566, Loss: 0.7839\n",
      "  Batch 800/3566, Loss: 0.8027\n",
      "  Batch 900/3566, Loss: 0.7289\n",
      "  Batch 1000/3566, Loss: 0.5922\n",
      "  Batch 1100/3566, Loss: 0.6423\n",
      "  Batch 1200/3566, Loss: 0.7903\n",
      "  Batch 1300/3566, Loss: 0.6807\n",
      "  Batch 1400/3566, Loss: 0.6208\n",
      "  Batch 1500/3566, Loss: 0.5792\n",
      "  Batch 1600/3566, Loss: 0.5950\n",
      "  Batch 1700/3566, Loss: 0.6337\n",
      "  Batch 1800/3566, Loss: 0.6245\n",
      "  Batch 1900/3566, Loss: 0.5897\n",
      "  Batch 2000/3566, Loss: 0.5489\n",
      "  Batch 2100/3566, Loss: 0.5902\n",
      "  Batch 2200/3566, Loss: 0.4886\n",
      "  Batch 2300/3566, Loss: 0.5271\n",
      "  Batch 2400/3566, Loss: 0.5490\n",
      "  Batch 2500/3566, Loss: 0.5620\n",
      "  Batch 2600/3566, Loss: 0.4886\n",
      "  Batch 2700/3566, Loss: 0.4223\n",
      "  Batch 2800/3566, Loss: 0.6377\n",
      "  Batch 2900/3566, Loss: 0.5675\n",
      "  Batch 3000/3566, Loss: 0.5387\n",
      "  Batch 3100/3566, Loss: 0.5077\n",
      "  Batch 3200/3566, Loss: 0.4594\n",
      "  Batch 3300/3566, Loss: 0.3752\n",
      "  Batch 3400/3566, Loss: 0.4423\n",
      "  Batch 3500/3566, Loss: 0.5415\n",
      "Epoch 1/10, Train Loss: 0.7409, Train Acc: 0.4317, Time: 250.59s\n",
      "Epoch 1/10, Test Loss: 0.7835, Test Acc: 0.5648\n",
      "  Batch 100/3566, Loss: 0.3800\n",
      "  Batch 200/3566, Loss: 0.4309\n",
      "  Batch 300/3566, Loss: 0.5078\n",
      "  Batch 400/3566, Loss: 0.5564\n",
      "  Batch 500/3566, Loss: 0.4770\n",
      "  Batch 600/3566, Loss: 0.3956\n",
      "  Batch 700/3566, Loss: 0.4958\n",
      "  Batch 800/3566, Loss: 0.4419\n",
      "  Batch 900/3566, Loss: 0.3875\n",
      "  Batch 1000/3566, Loss: 0.3883\n",
      "  Batch 1100/3566, Loss: 0.4498\n",
      "  Batch 1200/3566, Loss: 0.4360\n",
      "  Batch 1300/3566, Loss: 0.3982\n",
      "  Batch 1400/3566, Loss: 0.4478\n",
      "  Batch 1500/3566, Loss: 0.4775\n",
      "  Batch 1600/3566, Loss: 0.3823\n",
      "  Batch 1700/3566, Loss: 0.4535\n",
      "  Batch 1800/3566, Loss: 0.3412\n",
      "  Batch 1900/3566, Loss: 0.3416\n",
      "  Batch 2000/3566, Loss: 0.3186\n",
      "  Batch 2100/3566, Loss: 0.3600\n",
      "  Batch 2200/3566, Loss: 0.4914\n",
      "  Batch 2300/3566, Loss: 0.4149\n",
      "  Batch 2400/3566, Loss: 0.3158\n",
      "  Batch 2500/3566, Loss: 0.3937\n",
      "  Batch 2600/3566, Loss: 0.2998\n",
      "  Batch 2700/3566, Loss: 0.4797\n",
      "  Batch 2800/3566, Loss: 0.4187\n",
      "  Batch 2900/3566, Loss: 0.3391\n",
      "  Batch 3000/3566, Loss: 0.3027\n",
      "  Batch 3100/3566, Loss: 0.3041\n",
      "  Batch 3200/3566, Loss: 0.3866\n",
      "  Batch 3300/3566, Loss: 0.3242\n",
      "  Batch 3400/3566, Loss: 0.3699\n",
      "  Batch 3500/3566, Loss: 0.3749\n",
      "Epoch 2/10, Train Loss: 0.3942, Train Acc: 0.5923, Time: 250.38s\n",
      "Epoch 2/10, Test Loss: 0.5994, Test Acc: 0.6403\n",
      "  Batch 100/3566, Loss: 0.3969\n",
      "  Batch 200/3566, Loss: 0.2670\n",
      "  Batch 300/3566, Loss: 0.2867\n",
      "  Batch 400/3566, Loss: 0.3347\n",
      "  Batch 500/3566, Loss: 0.4875\n",
      "  Batch 600/3566, Loss: 0.2567\n",
      "  Batch 700/3566, Loss: 0.2912\n",
      "  Batch 800/3566, Loss: 0.3000\n",
      "  Batch 900/3566, Loss: 0.2801\n",
      "  Batch 1000/3566, Loss: 0.3356\n",
      "  Batch 1100/3566, Loss: 0.4106\n",
      "  Batch 1200/3566, Loss: 0.3218\n",
      "  Batch 1300/3566, Loss: 0.3060\n",
      "  Batch 1400/3566, Loss: 0.2473\n",
      "  Batch 1500/3566, Loss: 0.2933\n",
      "  Batch 1600/3566, Loss: 0.3479\n",
      "  Batch 1700/3566, Loss: 0.3410\n",
      "  Batch 1800/3566, Loss: 0.3289\n",
      "  Batch 1900/3566, Loss: 0.2550\n",
      "  Batch 2000/3566, Loss: 0.2445\n",
      "  Batch 2100/3566, Loss: 0.3353\n",
      "  Batch 2200/3566, Loss: 0.2527\n",
      "  Batch 2300/3566, Loss: 0.2659\n",
      "  Batch 2400/3566, Loss: 0.3307\n",
      "  Batch 2500/3566, Loss: 0.4178\n",
      "  Batch 2600/3566, Loss: 0.3202\n",
      "  Batch 2700/3566, Loss: 0.3509\n",
      "  Batch 2800/3566, Loss: 0.2474\n",
      "  Batch 2900/3566, Loss: 0.3132\n",
      "  Batch 3000/3566, Loss: 0.2920\n",
      "  Batch 3100/3566, Loss: 0.2857\n",
      "  Batch 3200/3566, Loss: 0.2791\n",
      "  Batch 3300/3566, Loss: 0.3284\n",
      "  Batch 3400/3566, Loss: 0.2831\n",
      "  Batch 3500/3566, Loss: 0.3339\n",
      "Epoch 3/10, Train Loss: 0.3132, Train Acc: 0.6483, Time: 252.13s\n",
      "Epoch 3/10, Test Loss: 0.5157, Test Acc: 0.6775\n",
      "  Batch 100/3566, Loss: 0.2591\n",
      "  Batch 200/3566, Loss: 0.2658\n",
      "  Batch 300/3566, Loss: 0.2300\n",
      "  Batch 400/3566, Loss: 0.3150\n",
      "  Batch 500/3566, Loss: 0.2621\n",
      "  Batch 600/3566, Loss: 0.2497\n",
      "  Batch 700/3566, Loss: 0.2850\n",
      "  Batch 800/3566, Loss: 0.2742\n",
      "  Batch 900/3566, Loss: 0.3178\n",
      "  Batch 1000/3566, Loss: 0.3069\n",
      "  Batch 1100/3566, Loss: 0.1817\n",
      "  Batch 1200/3566, Loss: 0.2458\n",
      "  Batch 1300/3566, Loss: 0.2753\n",
      "  Batch 1400/3566, Loss: 0.2483\n",
      "  Batch 1500/3566, Loss: 0.2910\n",
      "  Batch 1600/3566, Loss: 0.2288\n",
      "  Batch 1700/3566, Loss: 0.2383\n",
      "  Batch 1800/3566, Loss: 0.2739\n",
      "  Batch 1900/3566, Loss: 0.2377\n",
      "  Batch 2000/3566, Loss: 0.2449\n",
      "  Batch 2100/3566, Loss: 0.2804\n",
      "  Batch 2200/3566, Loss: 0.2974\n",
      "  Batch 2300/3566, Loss: 0.2810\n",
      "  Batch 2400/3566, Loss: 0.2626\n",
      "  Batch 2500/3566, Loss: 0.2610\n",
      "  Batch 2600/3566, Loss: 0.2807\n",
      "  Batch 2700/3566, Loss: 0.2096\n",
      "  Batch 2800/3566, Loss: 0.2643\n",
      "  Batch 2900/3566, Loss: 0.2082\n",
      "  Batch 3000/3566, Loss: 0.2868\n",
      "  Batch 3100/3566, Loss: 0.2245\n",
      "  Batch 3200/3566, Loss: 0.2307\n",
      "  Batch 3300/3566, Loss: 0.2052\n",
      "  Batch 3400/3566, Loss: 0.3292\n",
      "  Batch 3500/3566, Loss: 0.2264\n",
      "Epoch 4/10, Train Loss: 0.2666, Train Acc: 0.6816, Time: 250.03s\n",
      "Epoch 4/10, Test Loss: 0.4575, Test Acc: 0.6999\n",
      "  Batch 100/3566, Loss: 0.1661\n",
      "  Batch 200/3566, Loss: 0.1616\n",
      "  Batch 300/3566, Loss: 0.2407\n",
      "  Batch 400/3566, Loss: 0.2734\n",
      "  Batch 500/3566, Loss: 0.2269\n",
      "  Batch 600/3566, Loss: 0.2420\n",
      "  Batch 700/3566, Loss: 0.2459\n",
      "  Batch 800/3566, Loss: 0.2485\n",
      "  Batch 900/3566, Loss: 0.1991\n",
      "  Batch 1000/3566, Loss: 0.2108\n",
      "  Batch 1100/3566, Loss: 0.2773\n",
      "  Batch 1200/3566, Loss: 0.2799\n",
      "  Batch 1300/3566, Loss: 0.2185\n",
      "  Batch 1400/3566, Loss: 0.2460\n",
      "  Batch 1500/3566, Loss: 0.2516\n",
      "  Batch 1600/3566, Loss: 0.2023\n",
      "  Batch 1700/3566, Loss: 0.2202\n",
      "  Batch 1800/3566, Loss: 0.3195\n",
      "  Batch 1900/3566, Loss: 0.2376\n",
      "  Batch 2000/3566, Loss: 0.1692\n",
      "  Batch 2100/3566, Loss: 0.3452\n",
      "  Batch 2200/3566, Loss: 0.2876\n",
      "  Batch 2300/3566, Loss: 0.2147\n",
      "  Batch 2400/3566, Loss: 0.2607\n",
      "  Batch 2500/3566, Loss: 0.2280\n",
      "  Batch 2600/3566, Loss: 0.2418\n",
      "  Batch 2700/3566, Loss: 0.2664\n",
      "  Batch 2800/3566, Loss: 0.2373\n",
      "  Batch 2900/3566, Loss: 0.4088\n",
      "  Batch 3000/3566, Loss: 0.2677\n",
      "  Batch 3100/3566, Loss: 0.3093\n",
      "  Batch 3200/3566, Loss: 0.2742\n",
      "  Batch 3300/3566, Loss: 0.1347\n",
      "  Batch 3400/3566, Loss: 0.2502\n",
      "  Batch 3500/3566, Loss: 0.2891\n",
      "Epoch 5/10, Train Loss: 0.2351, Train Acc: 0.7051, Time: 254.76s\n",
      "Epoch 5/10, Test Loss: 0.4263, Test Acc: 0.7137\n",
      "  Batch 100/3566, Loss: 0.2314\n",
      "  Batch 200/3566, Loss: 0.1642\n",
      "  Batch 300/3566, Loss: 0.1807\n",
      "  Batch 400/3566, Loss: 0.1778\n",
      "  Batch 500/3566, Loss: 0.2788\n",
      "  Batch 600/3566, Loss: 0.3214\n",
      "  Batch 700/3566, Loss: 0.2362\n",
      "  Batch 800/3566, Loss: 0.2157\n",
      "  Batch 900/3566, Loss: 0.2185\n",
      "  Batch 1000/3566, Loss: 0.2204\n",
      "  Batch 1100/3566, Loss: 0.1707\n",
      "  Batch 1200/3566, Loss: 0.1965\n",
      "  Batch 1300/3566, Loss: 0.2068\n",
      "  Batch 1400/3566, Loss: 0.2162\n",
      "  Batch 1500/3566, Loss: 0.2303\n",
      "  Batch 1600/3566, Loss: 0.1627\n",
      "  Batch 1700/3566, Loss: 0.3196\n",
      "  Batch 1800/3566, Loss: 0.1769\n",
      "  Batch 1900/3566, Loss: 0.2055\n",
      "  Batch 2000/3566, Loss: 0.1919\n",
      "  Batch 2100/3566, Loss: 0.2091\n",
      "  Batch 2200/3566, Loss: 0.2148\n",
      "  Batch 2300/3566, Loss: 0.2040\n",
      "  Batch 2400/3566, Loss: 0.1735\n",
      "  Batch 2500/3566, Loss: 0.1791\n",
      "  Batch 2600/3566, Loss: 0.2100\n",
      "  Batch 2700/3566, Loss: 0.2350\n",
      "  Batch 2800/3566, Loss: 0.2388\n",
      "  Batch 2900/3566, Loss: 0.2359\n",
      "  Batch 3000/3566, Loss: 0.2247\n",
      "  Batch 3100/3566, Loss: 0.1981\n",
      "  Batch 3200/3566, Loss: 0.1694\n",
      "  Batch 3300/3566, Loss: 0.2087\n",
      "  Batch 3400/3566, Loss: 0.2587\n",
      "  Batch 3500/3566, Loss: 0.2371\n",
      "Epoch 6/10, Train Loss: 0.2120, Train Acc: 0.7220, Time: 248.72s\n",
      "Epoch 6/10, Test Loss: 0.4004, Test Acc: 0.7271\n",
      "  Batch 100/3566, Loss: 0.1925\n",
      "  Batch 200/3566, Loss: 0.1683\n",
      "  Batch 300/3566, Loss: 0.1460\n",
      "  Batch 400/3566, Loss: 0.1918\n",
      "  Batch 500/3566, Loss: 0.1760\n",
      "  Batch 600/3566, Loss: 0.1514\n",
      "  Batch 700/3566, Loss: 0.1187\n",
      "  Batch 800/3566, Loss: 0.1543\n",
      "  Batch 900/3566, Loss: 0.1899\n",
      "  Batch 1000/3566, Loss: 0.2288\n",
      "  Batch 1100/3566, Loss: 0.2170\n",
      "  Batch 1200/3566, Loss: 0.1464\n",
      "  Batch 1300/3566, Loss: 0.1887\n",
      "  Batch 1400/3566, Loss: 0.2112\n",
      "  Batch 1500/3566, Loss: 0.1772\n",
      "  Batch 1600/3566, Loss: 0.1614\n",
      "  Batch 1700/3566, Loss: 0.2192\n",
      "  Batch 1800/3566, Loss: 0.1823\n",
      "  Batch 1900/3566, Loss: 0.2236\n",
      "  Batch 2000/3566, Loss: 0.1935\n",
      "  Batch 2100/3566, Loss: 0.1957\n",
      "  Batch 2200/3566, Loss: 0.2176\n",
      "  Batch 2300/3566, Loss: 0.1983\n",
      "  Batch 2400/3566, Loss: 0.2414\n",
      "  Batch 2500/3566, Loss: 0.1938\n",
      "  Batch 2600/3566, Loss: 0.1480\n",
      "  Batch 2700/3566, Loss: 0.1894\n",
      "  Batch 2800/3566, Loss: 0.2504\n",
      "  Batch 2900/3566, Loss: 0.2257\n",
      "  Batch 3000/3566, Loss: 0.1978\n",
      "  Batch 3100/3566, Loss: 0.2035\n",
      "  Batch 3200/3566, Loss: 0.1807\n",
      "  Batch 3300/3566, Loss: 0.2041\n",
      "  Batch 3400/3566, Loss: 0.1704\n",
      "  Batch 3500/3566, Loss: 0.1667\n",
      "Epoch 7/10, Train Loss: 0.1941, Train Acc: 0.7363, Time: 252.41s\n",
      "Epoch 7/10, Test Loss: 0.3834, Test Acc: 0.7363\n",
      "  Batch 100/3566, Loss: 0.1648\n",
      "  Batch 200/3566, Loss: 0.1329\n",
      "  Batch 300/3566, Loss: 0.1971\n",
      "  Batch 400/3566, Loss: 0.1416\n",
      "  Batch 500/3566, Loss: 0.1677\n",
      "  Batch 600/3566, Loss: 0.1644\n",
      "  Batch 700/3566, Loss: 0.1491\n",
      "  Batch 800/3566, Loss: 0.1567\n",
      "  Batch 900/3566, Loss: 0.1960\n",
      "  Batch 1000/3566, Loss: 0.1787\n",
      "  Batch 1100/3566, Loss: 0.1717\n",
      "  Batch 1200/3566, Loss: 0.2287\n",
      "  Batch 1300/3566, Loss: 0.1734\n",
      "  Batch 1400/3566, Loss: 0.1763\n",
      "  Batch 1500/3566, Loss: 0.2164\n",
      "  Batch 1600/3566, Loss: 0.2034\n",
      "  Batch 1700/3566, Loss: 0.1391\n"
     ]
    }
   ],
   "source": [
    "tr.train_model(model, train_loader, test_loader, optimizer=torch.optim.Adam(model.parameters()), criterion= torch.nn.CrossEntropyLoss(), padding_fr=padding_fr, padding_en=padding_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
